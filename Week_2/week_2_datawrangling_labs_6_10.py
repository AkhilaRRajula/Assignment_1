# -*- coding: utf-8 -*-
"""Week_2_DataWrangling_labs_6-10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jbLcf3Sf3eZ7iZ8HPtw9PX4EqxEn7D68

# **Data Wrangling Lab**

Estimated time needed: **45 to 60** minutes

In this assignment you will be performing data wrangling.

## Objectives

In this lab you will perform the following:

-   Identify duplicate values in the dataset.

-   Remove duplicate values from the dataset.

-   Identify missing values in the dataset.

-   Impute the missing values in the dataset.

-   Normalize data in the dataset.

<hr>

## Hands on Lab

Import pandas module.
"""

import pandas as pd

"""Load the dataset into a dataframe.

"""

df = pd.read_csv("https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DA0321EN-SkillsNetwork/LargeData/m1_survey_data.csv")

"""## Finding duplicates

In this section you will identify duplicate values in the dataset.

Find how many duplicate rows exist in the dataframe.
"""

# Count the number of duplicate rows
num_duplicates = df.duplicated().sum()

print("Number of duplicate rows:", num_duplicates)

# Finding duplicates in the "Respondent" column
duplicate_count = df.duplicated(subset=['Respondent']).sum()

print("Number of duplicate values in the 'Respondent' column:", duplicate_count)

"""## Removing duplicates

Remove the duplicate rows from the dataframe.
"""

# Remove duplicate rows
df_cleaned = df.drop_duplicates()

# Print the shape of the cleaned DataFrame to verify the removal of duplicates
print("Shape of cleaned DataFrame:", df_cleaned.shape)

"""Verify if duplicates were actually dropped.

"""

# Remove duplicate rows
df_cleaned = df.drop_duplicates()

# Print the number of rows in the original DataFrame
print("Number of rows in original DataFrame:", len(df))

# Print the number of rows in the cleaned DataFrame
print("Number of rows in cleaned DataFrame:", len(df_cleaned))

# Check if duplicates were actually dropped
if len(df_cleaned) < len(df):
    print("Duplicates were dropped successfully.")
else:
    print("No duplicates were found in the DataFrame.")

"""## Finding Missing values

Find the missing values for all columns.
"""

# Find missing values for all columns
missing_values = df.isnull().sum()

print("Missing values for all columns:")
print(missing_values)

"""Find out how many rows are missing in the column 'WorkLoc'

"""

# Find missing values in the 'WorkLoc' column
missing_workloc = df['WorkLoc'].isnull().sum()

print("Number of rows missing in the 'WorkLoc' column:", missing_workloc)

"""## Imputing missing values

Find the  value counts for the column WorkLoc.
"""

# Find value counts for the 'WorkLoc' column
workloc_value_counts = df['WorkLoc'].value_counts()

print("Value counts for the 'WorkLoc' column:")
print(workloc_value_counts)

"""Identify the value that is most frequent (majority) in the WorkLoc column.

"""

# Find the most frequent value in the 'WorkLoc' column
most_frequent_workloc = df['WorkLoc'].value_counts().idxmax()

print("The most frequent value in the 'WorkLoc' column is:", most_frequent_workloc)

"""Impute (replace) all the empty rows in the column WorkLoc with the value that you have identified as majority.

"""

# Identify the most frequent value in the 'WorkLoc' column
most_frequent_workloc = df['WorkLoc'].value_counts().idxmax()

# Replace empty rows in the 'WorkLoc' column with the most frequent value
df['WorkLoc'].fillna(most_frequent_workloc, inplace=True)

# Verify the replacement
print("Value counts for the 'WorkLoc' column after replacement:")
print(df['WorkLoc'].value_counts())

"""After imputation there should ideally not be any empty rows in the WorkLoc column.

Verify if imputing was successful.
"""

# Identify the most frequent value in the 'WorkLoc' column
most_frequent_workloc = df['WorkLoc'].value_counts().idxmax()

# Replace empty rows in the 'WorkLoc' column with the most frequent value
df['WorkLoc'].fillna(most_frequent_workloc, inplace=True)

# Verify if there are any missing values in the 'WorkLoc' column
missing_workloc_after_imputation = df['WorkLoc'].isnull().sum()

if missing_workloc_after_imputation == 0:
    print("Imputation was successful. There are no empty rows in the 'WorkLoc' column.")
else:
    print("Imputation was not successful. There are still", missing_workloc_after_imputation, "empty rows in the 'WorkLoc' column.")

# Find the number of duplicate values in the 'Respondent' column
num_duplicates_respondent = df['Respondent'].duplicated().sum()

print("Number of duplicate values in the 'Respondent' column:", num_duplicates_respondent)

# Get the number of rows after removing duplicates
num_rows_after_removal = df_cleaned.shape[0]

print("Number of rows after removing duplicate rows:", num_rows_after_removal)

# Find the number of unique rows in the 'Respondent' column
num_unique_respondent = df_cleaned['Respondent'].nunique()

print("Number of unique rows in the 'Respondent' column after removing duplicate rows:", num_unique_respondent)

# Count the number of blank rows in the 'EdLevel' column after removing duplicate rows
num_blank_edlevel = df_cleaned['EdLevel'].isnull().sum()

print("Number of blank rows in the 'EdLevel' column after removing duplicate rows:", num_blank_edlevel)

# Count the number of missing rows in the 'Country' column after removing duplicate rows
num_missing_country = df_cleaned['Country'].isnull().sum()

print("Number of missing rows in the 'Country' column after removing duplicate rows:", num_missing_country)

# Identify the majority category under the 'Employment' column
majority_category_employment = df_cleaned['Employment'].value_counts().idxmax()

print("Majority category under the 'Employment' column:", majority_category_employment)

# Identify the category with the minimum number of rows under the 'UndergradMajor' column
min_category_undergrad_major = df_cleaned['UndergradMajor'].value_counts().idxmin()

print("Category with the minimum number of rows under the 'UndergradMajor' column:", min_category_undergrad_major)

"""## Normalizing data

"""

# Read the CSV file into a DataFrame
df = pd.read_csv("https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DA0321EN-SkillsNetwork/LargeData/m1_survey_data.csv")

"""There are two columns in the dataset that talk about compensation.

One is "CompFreq". This column shows how often a developer is paid (Yearly, Monthly, Weekly).

The other is "CompTotal". This column talks about how much the developer is paid per Year, Month, or Week depending upon his/her "CompFreq".

This makes it difficult to compare the total compensation of the developers.

In this section you will create a new column called 'NormalizedAnnualCompensation' which contains the 'Annual Compensation' irrespective of the 'CompFreq'.

Once this column is ready, it makes comparison of salaries easy.

<hr>

List out the various categories in the column 'CompFreq'
"""

# List out the various categories in the 'CompFreq' column
compfreq_categories = df['CompFreq'].unique()

print("Categories in the 'CompFreq' column:")
print(compfreq_categories)

"""Create a new column named 'NormalizedAnnualCompensation'. Use the hint given below if needed.

Double click to see the **Hint**.

<!--

Use the below logic to arrive at the values for the column NormalizedAnnualCompensation.

If the CompFreq is Yearly then use the exising value in CompTotal
If the CompFreq is Monthly then multiply the value in CompTotal with 12 (months in an year)
If the CompFreq is Weekly then multiply the value in CompTotal with 52 (weeks in an year)

-->
"""

# Define a function to convert compensation to an annual basis
def normalize_compensation(row):
    if row['CompFreq'] == 'Yearly':
        return row['CompTotal']
    elif row['CompFreq'] == 'Monthly':
        return row['CompTotal'] * 12
    elif row['CompFreq'] == 'Weekly':
        return row['CompTotal'] * 52
    else:
        return row['CompTotal']

# Apply the function to each row of the DataFrame to create the new column
df['NormalizedAnnualCompensation'] = df.apply(normalize_compensation, axis=1)

# Display the DataFrame with the new column
print(df.head())

# Count the number of unique values in the 'CompFreq' column
num_unique_values_compfreq = df['CompFreq'].nunique()

print("Number of unique values in the 'CompFreq' column:", num_unique_values_compfreq)

# Count the number of respondents being paid yearly
num_yearly_compensation = df_cleaned[df_cleaned['CompFreq'] == 'Yearly'].shape[0]

print("Number of respondents being paid yearly after removing duplicate rows:", num_yearly_compensation)

# Create the 'NormalizedAnnualCompensation' column
def normalize_compensation(row):
    if row['CompFreq'] == 'Yearly':
        return row['CompTotal']
    elif row['CompFreq'] == 'Monthly':
        return row['CompTotal'] * 12
    elif row['CompFreq'] == 'Weekly':
        return row['CompTotal'] * 52
    else:
        return None

df_cleaned['NormalizedAnnualCompensation'] = df_cleaned.apply(normalize_compensation, axis=1)

# Find the median of the 'NormalizedAnnualCompensation' column
median_normalized_annual_compensation = df_cleaned['NormalizedAnnualCompensation'].median()

print("Median NormalizedAnnualCompensation:", median_normalized_annual_compensation)